{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1TSwMlglJSqgjeUrRohvIQLvyUee51PnW","timestamp":1664793408279},{"file_id":"1XMyHqa5_ZEVBbEnzBeHfyoZHV9MPsrbC","timestamp":1664556449876}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from IPython.display import display, Javascript, Image\n","from google.colab.output import eval_js\n","from base64 import b64decode, b64encode\n","import cv2\n","import numpy as np\n","import PIL\n","import io\n","import html\n","import time"],"metadata":{"id":"UDeR7xe-jWZ6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# function to convert the JavaScript object into an OpenCV image\n","def js_to_image(js_reply):\n","  \"\"\"\n","  Params:\n","          js_reply: JavaScript object containing image from webcam\n","  Returns:\n","          img: OpenCV BGR image\n","  \"\"\"\n","  # decode base64 image\n","  image_bytes = b64decode(js_reply.split(',')[1])\n","  # convert bytes to numpy array\n","  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n","  # decode numpy array into OpenCV BGR image\n","  img = cv2.imdecode(jpg_as_np, flags=1)\n","\n","  return img\n","\n","# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n","def bbox_to_bytes(bbox_array):\n","  \"\"\"\n","  Params:\n","          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n","  Returns:\n","        bytes: Base64 image byte string\n","  \"\"\"\n","  # convert array into PIL image\n","  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n","  iobuf = io.BytesIO()\n","  # format bbox into png for return\n","  bbox_PIL.save(iobuf, format='png')\n","  # format return string\n","  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n","\n","  return bbox_bytes"],"metadata":{"id":"fjvffi5-jZO0"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"joxB6BB1iMZ4"},"outputs":[],"source":["def video_stream():\n","  js = Javascript('''\n","    var video;\n","    var div = null;\n","    var stream;\n","    var captureCanvas;\n","    var imgElement;\n","    var labelElement;\n","    \n","    var pendingResolve = null;\n","    var shutdown = false;\n","    \n","    function removeDom() {\n","       stream.getVideoTracks()[0].stop();\n","       video.remove();\n","       div.remove();\n","       video = null;\n","       div = null;\n","       stream = null;\n","       imgElement = null;\n","       captureCanvas = null;\n","       labelElement = null;\n","    }\n","    \n","    function onAnimationFrame() {\n","      if (!shutdown) {\n","        window.requestAnimationFrame(onAnimationFrame);\n","      }\n","      if (pendingResolve) {\n","        var result = \"\";\n","        if (!shutdown) {\n","          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n","          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n","        }\n","        var lp = pendingResolve;\n","        pendingResolve = null;\n","        lp(result);\n","      }\n","    }\n","    \n","    async function createDom() {\n","      if (div !== null) {\n","        return stream;\n","      }\n","\n","      div = document.createElement('div');\n","      div.style.border = '2px solid black';\n","      div.style.padding = '3px';\n","      div.style.width = '100%';\n","      div.style.maxWidth = '600px';\n","      document.body.appendChild(div);\n","      \n","      const modelOut = document.createElement('div');\n","      modelOut.innerHTML = \"<span>Status:</span>\";\n","      labelElement = document.createElement('span');\n","      labelElement.innerText = 'No data';\n","      labelElement.style.fontWeight = 'bold';\n","      modelOut.appendChild(labelElement);\n","      div.appendChild(modelOut);\n","           \n","      video = document.createElement('video');\n","      video.style.display = 'block';\n","      video.width = div.clientWidth - 6;\n","      video.setAttribute('playsinline', '');\n","      video.onclick = () => { shutdown = true; };\n","      stream = await navigator.mediaDevices.getUserMedia(\n","          {video: { facingMode: \"environment\"}});\n","      div.appendChild(video);\n","\n","      imgElement = document.createElement('img');\n","      imgElement.style.position = 'absolute';\n","      imgElement.style.zIndex = 1;\n","      imgElement.onclick = () => { shutdown = true; };\n","      div.appendChild(imgElement);\n","      \n","      const instruction = document.createElement('div');\n","      instruction.innerHTML = \n","          '<span style=\"color: red; font-weight: bold;\">' +\n","          'When finished, click here or on the video to stop this demo</span>';\n","      div.appendChild(instruction);\n","      instruction.onclick = () => { shutdown = true; };\n","      \n","      video.srcObject = stream;\n","      await video.play();\n","\n","      captureCanvas = document.createElement('canvas');\n","      captureCanvas.width = 640; //video.videoWidth;\n","      captureCanvas.height = 480; //video.videoHeight;\n","      window.requestAnimationFrame(onAnimationFrame);\n","      \n","      return stream;\n","    }\n","    async function stream_frame(label, imgData) {\n","      if (shutdown) {\n","        removeDom();\n","        shutdown = false;\n","        return '';\n","      }\n","\n","      var preCreate = Date.now();\n","      stream = await createDom();\n","      \n","      var preShow = Date.now();\n","      if (label != \"\") {\n","        labelElement.innerHTML = label;\n","      }\n","            \n","      if (imgData != \"\") {\n","        var videoRect = video.getClientRects()[0];\n","        imgElement.style.top = videoRect.top + \"px\";\n","        imgElement.style.left = videoRect.left + \"px\";\n","        imgElement.style.width = videoRect.width + \"px\";\n","        imgElement.style.height = videoRect.height + \"px\";\n","        imgElement.src = imgData;\n","      }\n","      \n","      var preCapture = Date.now();\n","      var result = await new Promise(function(resolve, reject) {\n","        pendingResolve = resolve;\n","      });\n","      shutdown = false;\n","      \n","      return {'create': preShow - preCreate, \n","              'show': preCapture - preShow, \n","              'capture': Date.now() - preCapture,\n","              'img': result};\n","    }\n","    ''')\n","\n","  display(js)\n","  \n","def video_frame(label, bbox):\n","  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n","  return data"]},{"cell_type":"code","source":["!pip install facenet_pytorch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iOnf4yqrxX4H","executionInfo":{"status":"ok","timestamp":1664793879651,"user_tz":-330,"elapsed":4636,"user":{"displayName":"Binod Binod","userId":"14980185732584765049"}},"outputId":"3426a47b-d6dd-4f10-d0a9-d97b21047b6d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting facenet_pytorch\n","  Downloading facenet_pytorch-2.5.2-py3-none-any.whl (1.9 MB)\n","\u001b[K     |████████████████████████████████| 1.9 MB 14.6 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from facenet_pytorch) (1.21.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from facenet_pytorch) (2.23.0)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from facenet_pytorch) (0.13.1+cu113)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from facenet_pytorch) (7.1.2)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->facenet_pytorch) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->facenet_pytorch) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->facenet_pytorch) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->facenet_pytorch) (2022.6.15)\n","Requirement already satisfied: torch==1.12.1 in /usr/local/lib/python3.7/dist-packages (from torchvision->facenet_pytorch) (1.12.1+cu113)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchvision->facenet_pytorch) (4.1.1)\n","Installing collected packages: facenet-pytorch\n","Successfully installed facenet-pytorch-2.5.2\n"]}]},{"cell_type":"code","source":["# importing libraries\n","\n","from facenet_pytorch import MTCNN, InceptionResnetV1, fixed_image_standardization, training\n","import torch\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader, SubsetRandomSampler\n","from torch import optim\n","from torch.optim.lr_scheduler import MultiStepLR\n","from torch.utils.tensorboard import SummaryWriter\n","import numpy as np\n","from PIL import Image\n","import cv2\n","import time\n","import os"],"metadata":{"id":"AwVyI2Gwj_2I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_dir = '/content/drive/MyDrive/photos'\n","\n","batch_size = 32\n","epochs = 20\n","workers = 0 if os.name == 'nt' else 8"],"metadata":{"id":"dIekN4K3cRHv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print('Running on device: {}'.format(device))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RsYGQ1exdE5v","executionInfo":{"status":"ok","timestamp":1664795209627,"user_tz":-330,"elapsed":10,"user":{"displayName":"Binod Binod","userId":"14980185732584765049"}},"outputId":"dd0ba54d-42bf-4cc7-edd0-fc43aa0f3d95"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running on device: cuda:0\n"]}]},{"cell_type":"code","source":["mtcnn = MTCNN(\n","    image_size=160, margin=0, min_face_size=20,\n","    thresholds=[0.6, 0.7, 0.7], factor=0.709, post_process=True,\n","    device=device\n",")"],"metadata":{"id":"3jVKdjT3dIVm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = datasets.ImageFolder(data_dir, transform=transforms.Resize((512, 512)))\n","dataset.samples = [\n","    (p, p.replace(data_dir, data_dir + '_cropped'))\n","        for p, _ in dataset.samples\n","]\n","        \n","loader = DataLoader(\n","    dataset,\n","    num_workers=workers,\n","    batch_size=batch_size,\n","    collate_fn=training.collate_pil\n",")\n","\n","for i, (x, y) in enumerate(loader):\n","    mtcnn(x, save_path=y)\n","    print('\\rBatch {} of {}'.format(i + 1, len(loader)), end='')\n","    \n","# Remove mtcnn to reduce GPU memory usage\n","del mtcnn"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dEIXUyvadOfN","executionInfo":{"status":"ok","timestamp":1664795213324,"user_tz":-330,"elapsed":1367,"user":{"displayName":"Binod Binod","userId":"14980185732584765049"}},"outputId":"5fa50b81-167e-4f9c-8aa5-d5cd105f911c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\rBatch 1 of 1"]}]},{"cell_type":"code","source":["resnet = InceptionResnetV1(\n","    classify=True,\n","    pretrained='vggface2',\n","    num_classes=len(dataset.class_to_idx)\n",").to(device)"],"metadata":{"id":"GA_1nTiKeDlX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer = optim.Adam(resnet.parameters(), lr=0.001)\n","scheduler = MultiStepLR(optimizer, [5, 10])\n","\n","trans = transforms.Compose([\n","    np.float32,\n","    transforms.ToTensor(),\n","    fixed_image_standardization\n","])\n","dataset = datasets.ImageFolder(data_dir + '_cropped', transform=trans)\n","img_inds = np.arange(len(dataset))\n","\n","np.random.shuffle(img_inds)\n","train_inds = img_inds[:int(0.8 * len(img_inds))]\n","val_inds = img_inds[int(0.8 * len(img_inds)):]\n","\n","train_loader = DataLoader(\n","    dataset,\n","    num_workers=workers,\n","    batch_size=batch_size,\n","    sampler=SubsetRandomSampler(train_inds)\n",")\n","val_loader = DataLoader(\n","    dataset,\n","    num_workers=workers,\n","    batch_size=batch_size,\n","    sampler=SubsetRandomSampler(val_inds)\n",")"],"metadata":{"id":"Piqpxcq6eJrF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img_inds"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AMtiA_d_fi_1","executionInfo":{"status":"ok","timestamp":1664795363818,"user_tz":-330,"elapsed":5,"user":{"displayName":"Binod Binod","userId":"14980185732584765049"}},"outputId":"6b3b42f8-ef49-4e50-b6f0-5dc379a43a28"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 9,  4, 10,  3, 14,  1, 11,  6,  7, 12,  0,  8, 13,  2,  5])"]},"metadata":{},"execution_count":66}]},{"cell_type":"code","source":["val_inds"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"un2qXfEVf1-t","executionInfo":{"status":"ok","timestamp":1664795364850,"user_tz":-330,"elapsed":7,"user":{"displayName":"Binod Binod","userId":"14980185732584765049"}},"outputId":"f29cb193-1f2c-4c13-a763-8c9ea11d741d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([13,  2,  5])"]},"metadata":{},"execution_count":67}]},{"cell_type":"code","source":["dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wlD0lJb3f7X5","executionInfo":{"status":"ok","timestamp":1664795370672,"user_tz":-330,"elapsed":1086,"user":{"displayName":"Binod Binod","userId":"14980185732584765049"}},"outputId":"0a7a678a-c7a1-4699-ba2f-2b2028fc2757"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dataset ImageFolder\n","    Number of datapoints: 15\n","    Root location: /content/drive/MyDrive/photos_cropped\n","    StandardTransform\n","Transform: Compose(\n","               <class 'numpy.float32'>\n","               ToTensor()\n","               <function fixed_image_standardization at 0x7fa82235d830>\n","           )"]},"metadata":{},"execution_count":68}]},{"cell_type":"code","source":["loss_fn = torch.nn.CrossEntropyLoss()\n","metrics = {\n","    'fps': training.BatchTimer(),\n","    'acc': training.accuracy\n","}"],"metadata":{"id":"prjYhBKsejxX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["writer = SummaryWriter()\n","writer.iteration, writer.interval = 0, 10\n","\n","print('\\n\\nInitial')\n","print('-' * 10)\n","resnet.eval()\n","training.pass_epoch(\n","    resnet, loss_fn, val_loader,\n","    batch_metrics=metrics, show_running=True, device=device,\n","    writer=writer\n",")\n","\n","for epoch in range(epochs):\n","    print('\\nEpoch {}/{}'.format(epoch + 1, epochs))\n","    print('-' * 10)\n","\n","    resnet.train()\n","    training.pass_epoch(\n","        resnet, loss_fn, train_loader, optimizer, scheduler,\n","        batch_metrics=metrics, show_running=True, device=device,\n","        writer=writer\n","    )\n","\n","    resnet.eval()\n","    training.pass_epoch(\n","        resnet, loss_fn, val_loader,\n","        batch_metrics=metrics, show_running=True, device=device,\n","        writer=writer\n","    )\n","\n","writer.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qg4fIVYSenS4","executionInfo":{"status":"ok","timestamp":1664795401779,"user_tz":-330,"elapsed":29004,"user":{"displayName":"Binod Binod","userId":"14980185732584765049"}},"outputId":"a9fe1079-0be2-4a7c-ec65-bd4b8f160679"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","Initial\n","----------\n","Valid |     1/1    | loss:    0.0415 | fps:    1.1316 | acc:    1.0000   \n","\n","Epoch 1/20\n","----------\n","Train |     1/1    | loss:    0.7629 | fps:   15.0399 | acc:    0.8333   \n","Valid |     1/1    | loss:    4.1428 | fps:    4.5806 | acc:    0.3333   \n","\n","Epoch 2/20\n","----------\n","Train |     1/1    | loss:    0.1099 | fps:   16.0743 | acc:    0.9167   \n","Valid |     1/1    | loss:   11.2723 | fps:    4.6356 | acc:    0.3333   \n","\n","Epoch 3/20\n","----------\n","Train |     1/1    | loss:    1.0631 | fps:   15.4535 | acc:    0.5833   \n","Valid |     1/1    | loss:   22.3633 | fps:    4.6669 | acc:    0.0000   \n","\n","Epoch 4/20\n","----------\n","Train |     1/1    | loss:    0.0668 | fps:   14.6380 | acc:    1.0000   \n","Valid |     1/1    | loss:   21.9864 | fps:    4.4736 | acc:    0.0000   \n","\n","Epoch 5/20\n","----------\n","Train |     1/1    | loss:    0.3101 | fps:   15.7486 | acc:    0.9167   \n","Valid |     1/1    | loss:   23.9265 | fps:    4.7111 | acc:    0.0000   \n","\n","Epoch 6/20\n","----------\n","Train |     1/1    | loss:    0.0053 | fps:   15.2854 | acc:    1.0000   \n","Valid |     1/1    | loss:   10.6387 | fps:    4.5686 | acc:    0.0000   \n","\n","Epoch 7/20\n","----------\n","Train |     1/1    | loss:    0.0185 | fps:   16.1564 | acc:    1.0000   \n","Valid |     1/1    | loss:    5.5608 | fps:    4.6589 | acc:    0.3333   \n","\n","Epoch 8/20\n","----------\n","Train |     1/1    | loss:    0.0099 | fps:   15.8786 | acc:    1.0000   \n","Valid |     1/1    | loss:    3.3380 | fps:    4.7365 | acc:    0.3333   \n","\n","Epoch 9/20\n","----------\n","Train |     1/1    | loss:    0.0221 | fps:   15.8878 | acc:    1.0000   \n","Valid |     1/1    | loss:    2.3468 | fps:    4.6798 | acc:    0.3333   \n","\n","Epoch 10/20\n","----------\n","Train |     1/1    | loss:    0.0298 | fps:   15.4634 | acc:    1.0000   \n","Valid |     1/1    | loss:    1.9511 | fps:    4.6971 | acc:    0.3333   \n","\n","Epoch 11/20\n","----------\n","Train |     1/1    | loss:    0.0057 | fps:   15.6139 | acc:    1.0000   \n","Valid |     1/1    | loss:    1.8606 | fps:    4.6858 | acc:    0.3333   \n","\n","Epoch 12/20\n","----------\n","Train |     1/1    | loss:    0.0102 | fps:   15.4948 | acc:    1.0000   \n","Valid |     1/1    | loss:    1.9021 | fps:    4.6571 | acc:    0.3333   \n","\n","Epoch 13/20\n","----------\n","Train |     1/1    | loss:    0.0164 | fps:   15.8512 | acc:    1.0000   \n","Valid |     1/1    | loss:    1.9837 | fps:    4.5857 | acc:    0.3333   \n","\n","Epoch 14/20\n","----------\n","Train |     1/1    | loss:    0.0199 | fps:   16.3233 | acc:    1.0000   \n","Valid |     1/1    | loss:    2.1031 | fps:    4.7827 | acc:    0.3333   \n","\n","Epoch 15/20\n","----------\n","Train |     1/1    | loss:    0.0091 | fps:   15.2450 | acc:    1.0000   \n","Valid |     1/1    | loss:    2.1679 | fps:    4.6324 | acc:    0.3333   \n","\n","Epoch 16/20\n","----------\n","Train |     1/1    | loss:    0.0170 | fps:   15.3150 | acc:    1.0000   \n","Valid |     1/1    | loss:    2.1873 | fps:    4.6747 | acc:    0.3333   \n","\n","Epoch 17/20\n","----------\n","Train |     1/1    | loss:    0.0160 | fps:   15.5959 | acc:    1.0000   \n","Valid |     1/1    | loss:    2.1709 | fps:    4.6234 | acc:    0.3333   \n","\n","Epoch 18/20\n","----------\n","Train |     1/1    | loss:    0.0214 | fps:   16.0211 | acc:    1.0000   \n","Valid |     1/1    | loss:    2.2407 | fps:    4.5971 | acc:    0.3333   \n","\n","Epoch 19/20\n","----------\n","Train |     1/1    | loss:    0.0072 | fps:   15.9001 | acc:    1.0000   \n","Valid |     1/1    | loss:    2.2312 | fps:    4.6410 | acc:    0.3333   \n","\n","Epoch 20/20\n","----------\n","Train |     1/1    | loss:    0.0056 | fps:   15.3490 | acc:    1.0000   \n","Valid |     1/1    | loss:    2.1988 | fps:    4.6921 | acc:    0.3333   \n"]}]},{"cell_type":"markdown","source":["## Purana Code"],"metadata":{"id":"sYIh9GOKeFYQ"}},{"cell_type":"code","source":["mtcnn = MTCNN(\n","    image_size=160, margin=0, min_face_size=20,\n","    thresholds=[0.6, 0.7, 0.7], factor=0.709, post_process=True,\n","    device=device\n",")"],"metadata":{"id":"UxvzBHlXkAQX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Read data from folder\n","\n","dataset = datasets.ImageFolder('/content/drive/MyDrive/photos') # photos folder path \n","idx_to_class = {i:c for c,i in dataset.class_to_idx.items()} # accessing names of peoples from folder names\n","\n","def collate_fn(x):\n","    return x[0]\n","\n","loader = DataLoader(dataset, collate_fn=collate_fn)\n","\n","name_list = [] # list of names corrospoing to cropped photos\n","embedding_list = [] # list of embeding matrix after conversion from cropped faces to embedding matrix using resnet\n","\n","for img, idx in loader:\n","    face, prob = mtcnn(img, return_prob=True) \n","    if face is not None and prob>0.92:\n","        emb = resnet(face.unsqueeze(0).cuda())\n","        embedding_list.append(emb.detach()) \n","        name_list.append(idx_to_class[idx])        \n","\n","# save data\n","data = [embedding_list, name_list] \n","torch.save(data, 'resnet_data.pt') # saving data.pt file"],"metadata":{"id":"Jhy-6ikqkBuu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Using webcam recognize face\n","from google.colab.patches import cv2_imshow\n","\n","# loading data.pt file\n","load_data = torch.load('resnet_data.pt') \n","embedding_list = load_data[0] \n","name_list = load_data[1] \n","\n","# cam = cv2.VideoCapture(0) \n","video_stream()\n","label_html = 'Capturing...'\n","# initialze bounding box to empty\n","bbox = ''\n","count = 0 \n","\n","while True:\n","    js_reply = video_frame(label_html, bbox)\n","    if not js_reply:\n","        break\n","\n","    # convert JS response to OpenCV Image\n","    frame = js_to_image(js_reply[\"img\"])\n","    frame_coverted = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","\n","    img = PIL.Image.fromarray(frame_coverted)\n","\n","    img_cropped_list, prob_list = mtcnn(img, return_prob=True) \n","    prob_list = [prob_list]    \n","    if img_cropped_list is not None:\n","        boxes, _ = mtcnn.detect(img)\n","                \n","        for i, prob in enumerate(prob_list):\n","            if prob>0.90:\n","                emb = resnet(img_cropped_list[i].unsqueeze(0)).reshape(-1).detach() \n","                \n","                dist_list = [] # list of matched distances, minimum distance is used to identify the person\n","                \n","                for idx, emb_db in enumerate(embedding_list):\n","                    dist = torch.dist(emb, emb_db).item()\n","                    dist_list.append(dist)\n","\n","                min_dist = min(dist_list) # get minumum dist value\n","                min_dist_idx = dist_list.index(min_dist) # get minumum dist index\n","                name = name_list[min_dist_idx] # get name corrosponding to minimum dist\n","                \n","                box = boxes[i] \n","                \n","                original_frame = frame.copy() # storing copy of frame before drawing on it\n","                \n","                if min_dist<200.90:\n","                    frame = cv2.putText(frame, name+' '+str(min_dist), (int(box[0]),int(box[1])), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0),1, cv2.LINE_AA)\n","                \n","                frame = cv2.rectangle(frame, (int(box[0]),int(box[1])) , (int(box[2]),int(box[3])), (255,0,0), 2)\n","\n","    cv2_imshow(frame)\n","        \n","    \n","    k = cv2.waitKey(1)\n","    if k%256==27: # ESC\n","        print('Esc pressed, closing...')\n","        break\n","        \n","    elif k%256==32: # space to save image\n","        print('Enter your name :')\n","        name = input()\n","        \n","        # create directory if not exists\n","        if not os.path.exists('photos/'+name):\n","            os.mkdir('photos/'+name)\n","            \n","        img_name = \"photos/{}/{}.jpg\".format(name, int(time.time()))\n","        cv2.imwrite(img_name, original_frame)\n","        print(\" saved: {}\".format(img_name))\n","        \n","        \n","# cam.release()\n","cv2.destroyAllWindows()"],"metadata":{"id":"xJU9BKagkGz3","colab":{"base_uri":"https://localhost:8080/","height":875},"executionInfo":{"status":"error","timestamp":1664796519824,"user_tz":-330,"elapsed":3733,"user":{"displayName":"Binod Binod","userId":"14980185732584765049"}},"outputId":"9a2392a7-784d-4336-9baa-e1d70f523d90"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    var video;\n","    var div = null;\n","    var stream;\n","    var captureCanvas;\n","    var imgElement;\n","    var labelElement;\n","    \n","    var pendingResolve = null;\n","    var shutdown = false;\n","    \n","    function removeDom() {\n","       stream.getVideoTracks()[0].stop();\n","       video.remove();\n","       div.remove();\n","       video = null;\n","       div = null;\n","       stream = null;\n","       imgElement = null;\n","       captureCanvas = null;\n","       labelElement = null;\n","    }\n","    \n","    function onAnimationFrame() {\n","      if (!shutdown) {\n","        window.requestAnimationFrame(onAnimationFrame);\n","      }\n","      if (pendingResolve) {\n","        var result = \"\";\n","        if (!shutdown) {\n","          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n","          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n","        }\n","        var lp = pendingResolve;\n","        pendingResolve = null;\n","        lp(result);\n","      }\n","    }\n","    \n","    async function createDom() {\n","      if (div !== null) {\n","        return stream;\n","      }\n","\n","      div = document.createElement('div');\n","      div.style.border = '2px solid black';\n","      div.style.padding = '3px';\n","      div.style.width = '100%';\n","      div.style.maxWidth = '600px';\n","      document.body.appendChild(div);\n","      \n","      const modelOut = document.createElement('div');\n","      modelOut.innerHTML = \"<span>Status:</span>\";\n","      labelElement = document.createElement('span');\n","      labelElement.innerText = 'No data';\n","      labelElement.style.fontWeight = 'bold';\n","      modelOut.appendChild(labelElement);\n","      div.appendChild(modelOut);\n","           \n","      video = document.createElement('video');\n","      video.style.display = 'block';\n","      video.width = div.clientWidth - 6;\n","      video.setAttribute('playsinline', '');\n","      video.onclick = () => { shutdown = true; };\n","      stream = await navigator.mediaDevices.getUserMedia(\n","          {video: { facingMode: \"environment\"}});\n","      div.appendChild(video);\n","\n","      imgElement = document.createElement('img');\n","      imgElement.style.position = 'absolute';\n","      imgElement.style.zIndex = 1;\n","      imgElement.onclick = () => { shutdown = true; };\n","      div.appendChild(imgElement);\n","      \n","      const instruction = document.createElement('div');\n","      instruction.innerHTML = \n","          '<span style=\"color: red; font-weight: bold;\">' +\n","          'When finished, click here or on the video to stop this demo</span>';\n","      div.appendChild(instruction);\n","      instruction.onclick = () => { shutdown = true; };\n","      \n","      video.srcObject = stream;\n","      await video.play();\n","\n","      captureCanvas = document.createElement('canvas');\n","      captureCanvas.width = 640; //video.videoWidth;\n","      captureCanvas.height = 480; //video.videoHeight;\n","      window.requestAnimationFrame(onAnimationFrame);\n","      \n","      return stream;\n","    }\n","    async function stream_frame(label, imgData) {\n","      if (shutdown) {\n","        removeDom();\n","        shutdown = false;\n","        return '';\n","      }\n","\n","      var preCreate = Date.now();\n","      stream = await createDom();\n","      \n","      var preShow = Date.now();\n","      if (label != \"\") {\n","        labelElement.innerHTML = label;\n","      }\n","            \n","      if (imgData != \"\") {\n","        var videoRect = video.getClientRects()[0];\n","        imgElement.style.top = videoRect.top + \"px\";\n","        imgElement.style.left = videoRect.left + \"px\";\n","        imgElement.style.width = videoRect.width + \"px\";\n","        imgElement.style.height = videoRect.height + \"px\";\n","        imgElement.src = imgData;\n","      }\n","      \n","      var preCapture = Date.now();\n","      var result = await new Promise(function(resolve, reject) {\n","        pendingResolve = resolve;\n","      });\n","      shutdown = false;\n","      \n","      return {'create': preShow - preCreate, \n","              'show': preCapture - preShow, \n","              'capture': Date.now() - preCapture,\n","              'img': result};\n","    }\n","    "]},"metadata":{}},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-80-f3c85f2bff38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0.90\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                 \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_cropped_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mdist_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# list of matched distances, minimum distance is used to identify the person\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/facenet_pytorch/models/inception_resnet_v1.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0mBatch\u001b[0m \u001b[0mof\u001b[0m \u001b[0membedding\u001b[0m \u001b[0mvectors\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmultinomial\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \"\"\"\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_1a\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_2a\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_2b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/facenet_pytorch/models/inception_resnet_v1.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    453\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 454\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [32, 3, 3, 3], expected input[1, 1, 160, 160] to have 3 channels, but got 1 channels instead"]}]},{"cell_type":"code","source":[],"metadata":{"id":"zgJnk7STmtc1"},"execution_count":null,"outputs":[]}]}